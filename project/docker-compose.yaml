

services:

  rag-ai-agent-service:
    build:
      context: ./../rag-ai-agent
      dockerfile: ./../rag-ai-agent/rag-ai-agent.dockerfile
    restart: always
    deploy:
      mode: replicated
      replicas: 1
    depends_on:
      - weaviate

  front-end-service:
    build:
      context: ./../front-end
      dockerfile: ./../front-end/front-end-service.dockerfile
    ports:
      - "4000:4000"
    depends_on:
      - rag-ai-agent-service
      - ollama-service


  ollama-service:
    image: ollama/ollama:latest
    container_name: ollama-service
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
    depends_on:
      - ollama-init
    healthcheck:
      test: ["CMD", "sh", "-c", "ollama list | grep llama2"]
      interval: 10s
      timeout: 10s
      retries: 12

  # Init container to pull llama2 before server is healthy
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    volumes:
      - ./ollama-data:/root/.ollama
    entrypoint: ["/bin/sh", "-c"]
    command: >
      i=0;
      until curl -s http://ollama-service:11434/api/tags >/dev/null || [ $$i -ge 30 ]; do
        i=$$((i+1)); sleep 1;
      done;
      echo "Pulling llama2...";
      ollama pull llama2;
      echo "llama2 pulled successfully."

  weaviate:
    image: semitechnologies/weaviate:1.19.0
    container_name: weaviate
    hostname: weaviate
    ports:
      - "8080:8080"

    environment:
      QUERY_DEFAULTS_LIMIT: 20
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"

      DEFAULT_VECTORIZER_MODULE: "text2vec-openai"
      ENABLE_MODULES: "text2vec-openai"
      OPENAI_EMBEDDING_MODEL: "text-embedding-3-small"
      OPENAI_APIKEY: "{OPENAI_APIKEY}"
      UUID_ALLOW_NON_UUID: "true"
      CLUSTER_HOSTNAME: "weaviate"
      CLUSTER_JOIN: ""
 
    volumes:
      - ./weaviate_data:/var/lib/weaviate
